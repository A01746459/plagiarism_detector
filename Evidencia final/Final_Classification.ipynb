{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enThjnbRKIaP"
      },
      "source": [
        "# Base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1n9qLMdSn0V"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3YzujC_OoNF",
        "outputId": "570b59f5-e025-48d4-b039-a71be28710d5"
      },
      "outputs": [],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5WGLuXMdPKI",
        "outputId": "164f372e-9588-4be7-cc5f-24a6f0eb4e2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "drive.mount(\"/content/gdrive\")\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaMK2ZRFdTiL"
      },
      "outputs": [],
      "source": [
        "suspicious_folder = \"/content/gdrive/MyDrive/project_bert/suspicious\"\n",
        "dataset_folder = \"/content/gdrive/MyDrive/project_bert/dataset\"\n",
        "\n",
        "model_name = 'sentence-transformers/bert-base-nli-mean-tokens'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModel.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_zM8-uqT5a0"
      },
      "outputs": [],
      "source": [
        "def get_mean_pooling_embeddings(text, tokenizer, model):\n",
        "    '''\n",
        "    Esta función recibe un texto, un tokenizador y un modelo y devuelve los embeddings\n",
        "    de la capa de pooling media.\n",
        "\n",
        "    Args:\n",
        "    text: str: texto a tokenizar\n",
        "    tokenizer: tokenizer: tokenizador\n",
        "    model: model: modelo\n",
        "\n",
        "    Returns:\n",
        "    mean_pooled: torch.tensor: embedding promedio de las palabras del texto\n",
        "    \n",
        "    '''\n",
        "    tokens = tokenizer.encode_plus(text, max_length=128,\n",
        "                                    truncation=True, padding='max_length',\n",
        "                                    return_tensors='pt')\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokens)\n",
        "        embeddings = outputs.last_hidden_state\n",
        "        attention_mask = tokens['attention_mask']\n",
        "        mask = attention_mask.unsqueeze(-1).expand(embeddings.shape).float()\n",
        "        mask_embeddings = embeddings * mask\n",
        "        summed = torch.sum(mask_embeddings, 1)\n",
        "        counts = torch.clamp(mask.sum(1), min=1e-9)\n",
        "        mean_pooled = summed / counts\n",
        "        return mean_pooled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY9qyTBQ4AnN"
      },
      "outputs": [],
      "source": [
        "def load_texts_from_folder(folder_path):\n",
        "    '''\n",
        "    Esta funcion carga los textos de un directorio en una lista.\n",
        "\n",
        "    :param folder_path: str, ruta al directorio que contiene los textos.\n",
        "    :return: list, lista con los textos cargados.\n",
        "\n",
        "    '''\n",
        "    texts = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        filepath = os.path.join(folder_path, filename)\n",
        "        if os.path.isfile(filepath):\n",
        "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "                try:\n",
        "                    text = file.read()\n",
        "                    texts.append(text)\n",
        "                except UnicodeDecodeError as e:\n",
        "                    print(f\"Error al decodificar el archivo {filepath}: {e}\")\n",
        "    return texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qb4K3mZF4OuW"
      },
      "outputs": [],
      "source": [
        "suspicious_texts = load_texts_from_folder(suspicious_folder)\n",
        "dataset_texts = load_texts_from_folder(dataset_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yf1WIS5LlFuO",
        "outputId": "a072d748-f419-4ddc-91ee-acc17c40d147"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'FID-05.txt': {'suspicious': 'Internet of Things (IoT) based remote healthcare applications provided fast and preventative medical services to the patients at risk. However, predicting heart disease was a complex task and diagnosis results were rarely accurate. To address this issue, a novel Recommendation System for Cardiovascular Disease Prediction Using IoT Network (DEEP-CARDIO) has been proposed for providing prior diagnosis, treatment, and dietary recommendations for cardiac diseases. Initially, the physiological data were collected from the patient’s remotely by using the four bio sensors such as ECG sensor, Pressure sensor, Pulse sensor and Glucose sensor. An Arduino controller received the collected data from the IoT sensors to predict and diagnose the disease. A cardiovascular disease prediction model was implemented by using BiGRU (Bidirectional-Gated Recurrent Unit) attention model which diagnosed the cardiovascular disease and classified into five available cardiovascular classes. The recommendation system provided physical and dietary recommendations to cardiac patients based on the classified data, via user mobile application. The performance of the DEEP-CARDIO was validated by Cloud Simulator (CloudSim) using the real-time Framingham’s and Statlog heart disease dataset. The proposed DEEP CARDIO method achieved an overall accuracy of 99.90% whereas, the MABC-SVM, HCBDA and MLbPM method achieved 86.91%, 88.65% and 93.63% respectively.\\n', 'dataset': {'org-085.txt': 'Internet of Things (IoT) based remote healthcare applications provide fast and preventative medical services to the patients at risk. However, predicting heart disease is a complex task and diagnosis results are rarely accurate. To address this issue, a novel Recommendation System for Cardiovascular Disease Prediction Using IoT Network (DEEP-CARDIO) has been proposed for providing prior diagnosis, treatment, and dietary recommendations for cardiac diseases. Initially, the physiological data are collected from the patient’s remotely by using the four bio sensors such as ECG sensor, Pressure sensor, Pulse sensor and Glucose sensor. An Arduino controller receives the collected data from the IoT sensors to predict and diagnose the disease. A cardiovascular disease prediction model is implemented by using BiGRU (Bidirectional-Gated Recurrent Unit) attention model which diagnose the cardiovascular disease and classify into five available cardiovascular classes. The recommendation system provides physical and dietary recommendations to cardiac patients based on the classified data, via user mobile application. The performance of the DEEP-CARDIO is validated by Cloud Simulator (CloudSim) using the real-time Framingham’s and Statlog heart disease dataset. The proposed DEEP CARDIO method achieves an overall accuracy of 99.90% whereas, the MABC-SVM, HCBDA and MLbPM method achieves 86.91%, 88.65% and 93.63% respectively.'}}, 'FID-02.txt': {'suspicious': 'Machine learning (ML) is a form of artificial intelligence which is placed to transform the twenty-first century. Rapid, recent progress in its underlying architecture and algorithms and growth in the size of datasets have led to increasing computer competence across a range of fields. For these algorithms, choosing a proper fitness function plays an important role. These include driving a vehicle, language translation, chatbots and beyond human performance at complex board games such as Go. Here, we review the fundamentals and algorithms behind machine learning and highlight specific approaches to learning and optimisation. We then summarise the applications of ML to medicine. The fitness function orients the searching strategy of the algorithms to obtain best solutions. Feature selection (FS) is an optimisation problem that reduces the dimension of the dataset and increases the performance of the machine learning algorithms and classification through the selection of the optimal subset features and elimination of the redundant features. \\n', 'dataset': {'org-104.txt': 'Machine learning (ML) is a form of artificial intelligence which is placed to transform the twenty-first century. Rapid, recent progress in its underlying architecture and algorithms and growth in the size of datasets have led to increasing computer competence across a range of fields. These include driving a vehicle, language translation, chatbots and beyond human performance at complex board games such as Go. Here, we review the fundamentals and algorithms behind machine learning and highlight specific approaches to learning and optimisation. We then summarise the applications of ML to medicine. In particular, we showcase recent diagnostic performances, and caveats, in the fields of dermatology, radiology, pathology and general microscopy.', 'org-102.txt': 'With the unprecedented advancement of data aggregation and deep learning algorithms, artificial intelligence (AI) and machine learning (ML) are poised to transform the practice of medicine. The field of orthopedics, in particular, is uniquely suited to harness the power of big data, and in doing so provide critical insight into elevating the many facets of care provided by orthopedic surgeons. The purpose of this review is to critically evaluate the recent and novel literature regarding ML in the field of orthopedics and to address its potential impact on the future of musculoskeletal care.', 'org-056.txt': \"INTRODUCTION: In recent years, there has been a convergence between Artificial Intelligence and neuroscience, particularly in studying the brain and developing treatments for neurological disorders. Artificial neural networks and deep learning provide valuable insights into neural processing and brain functioning. Recent research tries to explain how neural processes influence an individual's happiness. OBJECTIVES: To evaluate the interaction between neuroscience and happiness based on the advances in Artificial Intelligence. METHODS: A bibliometric analysis was performed with articles from the Scopus database in 2013-2023; likewise, the VOSviewer was used for information processing. RESULTS A total of 603 articles were obtained, and it is evident that the most significant scientific production is centered in the United States (184), United Kingdom (74), and China (73). Three clusters are generated from the Co-occurrence - Author Keywords analysis. The first cluster, red, is related to Artificial Intelligence applications for predicting happiness; the second cluster, green, is associated with Artificial Intelligence tools in neuroscience; and the third cluster, blue, is related to neuroscience in psychology. CONCLUSION: Neuroscience research has made significant leaps in understanding mental processes such as emotions and consciousness. Neuroscience has encountered happiness and is opening up to an approach that seeks evidence to understand people's well-being supported by Artificial Intelligence.\", 'org-101.txt': 'The lifestyle of modern society has changed significantly with the emergence of artificial intelligence (AI), machine learning (ML), and deep learning (DL) technologies in recent years. Artificial intelligence is a multidimensional technology with various components such as advanced algorithms, ML and DL. Together, AI, ML, and DL are expected to provide automated devices to ophthalmologists for early diagnosis and timely treatment of ocular disorders in the near future. In fact, AI, ML, and DL have been used in ophthalmic setting to validate the diagnosis of diseases, read images, perform corneal topographic mapping and intraocular lens calculations. Diabetic retinopathy (DR), age-related macular degeneration (AMD), and glaucoma are the 3 most common causes of irreversible blindness on a global scale. Ophthalmic imaging provides a way to diagnose and objectively detect the progression of a number of pathologies including DR, AMD, glaucoma, and other ophthalmic disorders. There are 2 methods of imaging used as diagnostic methods in ophthalmic practice: fundus digital photography and optical coherence tomography (OCT). Of note, OCT has become the most widely used imaging modality in ophthalmology settings in the developed world. Changes in population demographics and lifestyle, extension of average lifespan, and the changing pattern of chronic diseases such as obesity, diabetes, DR, AMD, and glaucoma create a rising demand for such images. Furthermore, the limitation of availability of retina specialists and trained human graders is a major problem in many countries. Consequently, given the current population growth trends, it is inevitable that analyzing such images is time-consuming, costly, and prone to human error. Therefore, the detection and treatment of DR, AMD, glaucoma, and other ophthalmic disorders through unmanned automated applications system in the near future will be inevitable. We provide an overview of the potential impact of the current AI, ML, and DL methods and their applications on the early detection and treatment of DR, AMD, glaucoma, and other ophthalmic diseases.'}}, 'FID-03.txt': {'suspicious': 'At present, the application of Artificial Intelligence (AI) in industrial control, smart home and other fields has received good response. However, AI technology has certain requirements for computer performance, and also faces problems in network security, data analysis, human-computer interaction, etc. At present, the visual platform of embedded system has achieved remarkable results in practical applications, but its development has been seriously hampered by problems such as low overall development efficiency and unstable system performance. The test results showed that when other conditions were the same, students and experts had 83.5% and 90% positive evaluations of System X, and 16.5% and 10% negative evaluations respectively. This paper designed an EP Vision System (VS) based on AI technology. The platform combined the embedded hardware design with the Support Vector Machine (SVM) algorithm to realize the intelligent robot interaction and target detection functions. It showed the positive relationship between AI technology and EP VS. The proportion of positive evaluation of System X was much higher than that of System Y, which indicated that System X can meet the actual application requirements and improve the system recognition efficiency to a certain extent. However, their positive evaluation of System Y only accounted for 19% and 4%, while the negative evaluation accounted for 81% and 96%. However, their positive evaluation of System Y only accounted for 19% and 4%, while the negative evaluation accounted for 81% and 96%. \\n', 'dataset': {'org-016.txt': 'At present, the application of Artificial Intelligence (AI) in industrial control, smart home and other fields has received good response. However, AI technology has certain requirements for computer performance, and also faces problems in network security, data analysis, human-computer interaction, etc. At present, the visual platform of embedded system has achieved remarkable results in practical applications, but its development has been seriously hampered by problems such as low overall development efficiency and unstable system performance. This paper designed an EP Vision System (VS) based on AI technology. The platform combined the embedded hardware design with the Support Vector Machine (SVM) algorithm to realize the intelligent robot interaction and target detection functions. The test results showed that when other conditions were the same, students and experts had 83.5% and 90% positive evaluations of System X, and 16.5% and 10% negative evaluations respectively. However, their positive evaluation of System Y only accounted for 19% and 4%, while the negative evaluation accounted for 81% and 96%. The proportion of positive evaluation of System X was much higher than that of System Y, which indicated that System X can meet the actual application requirements and improve the system recognition efficiency to a certain extent. It showed the positive relationship between AI technology and EP VS.'}}, 'FID-04.txt': {'suspicious': 'Interactive software agents, such as chatbots, are progressively being used in the area of health and well-being. The definition, implemented in Web Ontology Language (OWL), may serve as an automated tool, enabling systems to recognize empathy in interactions—be it an interactive agent evaluating its own empathic performance or an intelligent system assessing the empathic capability of its interlocutors. In the current state-of-the-art, there are no tools to do that. We present the potential of the formal definition in a controlled user-study by applying it as a tool for assessing empathy in two state-of-the-art health and well-being chatbots; Replika and Wysa. Our findings suggest that our definition captures necessary conditions for assessing empathy in interactive agents, and how it can uncover and explain trends in changing perceptions of empathy over time. Based on a systematic literature review and a qualitative analysis of recent approaches to empathy in interactive agents for health and well-being, a formal definition—an ontology—of empathy is developed. In order to understand empathic capabilities in interactive software agents, we need a precise notion of empathy. The literature discusses a variety of definitions of empathy, but there is no consensus of a formal definition. In such applications, where agents engage with users in interpersonal conversations for, e.g., coaching, comfort or behavior-change interventions, there is an increased need for understanding agents’ empathic capabilities. \\n', 'dataset': {'org-045.txt': 'Interactive software agents, such as chatbots, are progressively being used in the area of health and well-being. In such applications, where agents engage with users in interpersonal conversations for, e.g., coaching, comfort or behavior-change interventions, there is an increased need for understanding agents’ empathic capabilities. In the current state-of-the-art, there are no tools to do that. In order to understand empathic capabilities in interactive software agents, we need a precise notion of empathy. The literature discusses a variety of definitions of empathy, but there is no consensus of a formal definition. Based on a systematic literature review and a qualitative analysis of recent approaches to empathy in interactive agents for health and well-being, a formal definition—an ontology—of empathy is developed. We present the potential of the formal definition in a controlled user-study by applying it as a tool for assessing empathy in two state-of-the-art health and well-being chatbots; Replika and Wysa. Our findings suggest that our definition captures necessary conditions for assessing empathy in interactive agents, and how it can uncover and explain trends in changing perceptions of empathy over time. The definition, implemented in Web Ontology Language (OWL), may serve as an automated tool, enabling systems to recognize empathy in interactions—be it an interactive agent evaluating its own empathic performance or an intelligent system assessing the empathic capability of its interlocutors.\\n'}}, 'FID-06.txt': {'suspicious': 'This study is conducted to investigate the empathy between human-chatbot interactions among computer science students at Uppsala University, Sweden. This is done by exploring how participants perceived anthropomorphic chatbots as machines or humans, the existence of verbal abuse during human-chatbot interactions, and the expectation of chatbot helpfulness depending on gender dynamics. A semi-structured interview methodology with five students were conducted for qualitative data collection. The collected data is manually analyzed using thematic analysis. The results of this study find that there was empathy in human-chatbot interaction, regardless of whether participants perceive anthropomorphic chatbots as humans or machines. However, the level of empathy was generally low as participants become frustrated when they were dissatisfied with the response of chatbots and exit the chatbots without expressing their frustration. They usually forgot their frustration and came again with other questions another time. The study also showed that participants may expect more help and politeness if chatbots are more likely to be female.\\n', 'dataset': {'org-043.txt': 'This study was conducted to investigate the empathy between human chatbot interactions amongcomputer science students at Uppsala University, Sweden. This was done by exploring howparticipants perceive anthropomorphic chatbots as machines or humans, the existence of verbalabuse during human chatbot interactions, and the expectation of chatbot helpfulness dependingon gender dynamics. A semi-structured interview methodology with five students was conductedfor qualitative data collection. The collected data was manually analyzed using thematic analysis.The results of this study found that there is empathy in human chatbot interaction, regardless ofwhether participants perceive anthropomorphic chatbots as humans or machines. However, thelevel of empathy is generally low as participants frustrate when they are dissatisfied with theresponse of chatbots and exit the chatbots without expressing their frustration, and they usuallyforget their frustration and come again with other questions another time. The study also showsthat participants might expect more help and politeness if chatbots are more likely to be female.'}}, 'FID-01.txt': {'suspicious': '\\ufeffThis article delves into the intricacies of adaptive fuzzy event-triggered formation tracking control for nonholonomic multirobot systems characterized by infinite actuator faults and range constraints. Traditional cheating detection methods have many disadvantages, such as difficult to detect covert equipment cheating, multi-source cheating, difficult to distinguish plagiarists from plagiarists, difficult to distinguish plagiarists from victims, or plagiarism from coincidences. To address these issues, we leverage the power of fuzzy logic systems (FLSs) and employ adaptive methods to approximate unknown nonlinear functions and uncertain parameters present in robotic dynamics. In the course of information exploration, the problems of collision avoidance and connectivity maintenance are ever present due to limitations of distance and visual fields. In this paper, the concept of knowledge point mastery Index is introduced to measure students’ mastery of a certain knowledge point, and a test method of cheating based on improved cognitive diagnostic model is proposed. Furthermore, to reduce the number of controller executions and compensate for any effect arising from infinite actuator failures, robots engage with their leader at the moment of actuator faults using fewer network communication resources yet maintain uninterrupted tracking of the desired trajectory generated by the leader. We guarantee that all signals are semi-global uniformly ultimately bounded (SGUUB). Ultimately, we demonstrate the practical feasibility of the ETFT control strategy for nonholonomic multirobot systems.The experiments show that the precision and recall rate of this method are significantly higher than those of the method based on the false-same rate, the method based on the false-same rate and the right-same rate and the method based on the Person-Fit index.\\n', 'dataset': {'org-076.txt': '\\ufeffThis article delves into the intricacies of adaptive fuzzy event-triggered formation tracking control for nonholonomic multirobot systems characterized by infinite actuator faults and range constraints. To address these issues, we leverage the power of fuzzy logic systems (FLSs) and employ adaptive methods to approximate unknown nonlinear functions and uncertain parameters present in robotic dynamics. In the course of information exploration, the problems of collision avoidance and connectivity maintenance are ever present due to limitations of distance and visual fields. In this regard, we introduce a general barrier function and prescribed performance methodology to tackle constrained range impediments effectively. Furthermore, to reduce the number of controller executions and compensate for any effect arising from infinite actuator failures, robots engage with their leader at the moment of actuator faults using fewer network communication resources yet maintain uninterrupted tracking of the desired trajectory generated by the leader. With the aid of the dynamic surface technology, we propose a decentralized adaptive event-triggering fault-tolerant (ETFT) formation control strategy. We guarantee that all signals are semi-global uniformly ultimately bounded (SGUUB). Ultimately, we demonstrate the practical feasibility of the ETFT control strategy for nonholonomic multirobot systems.'}}, 'FID-09.txt': {'suspicious': 'Drug designing and development represent crucial areas of research for pharmaceutical companies and chemical scientists. However, challenges such as low efficacy, off-target delivery, time consumption, and high cost hinder progress in drug design and discovery. Additionally, the complexity and volume of data from genomics, proteomics, microarray data, and clinical trials pose significant obstacles in the drug discovery pipeline. Artificial intelligence (AI) and machine learning (ML) technologies have revolutionized drug discovery and development, particularly through the use of artificial neural networks and deep learning algorithms. These technologies have modernized various processes in drug discovery, including peptide synthesis, structure-based virtual screening, ligand-based virtual screening, toxicity prediction, drug monitoring and release, pharmacophore modeling, quantitative structure–activity relationship, drug repositioning, polypharmacology, and physiochemical activity. Historical evidence supports the implementation of AI and deep learning in drug discovery. Furthermore, novel data mining, curation, and management techniques have provided critical support to newly developed modeling algorithms. In summary, advancements in AI and deep learning offer significant opportunities for rational drug design and discovery, ultimately benefiting mankind.Drug designing and development is an important area of research for pharmaceutical companies and chemical scientists. However, low efficacy, off-target delivery, time consumption, and high cost impose a hurdle and challenges that impact drug design and discovery. Further, complex and big data from genomics, proteomics, microarray data, and clinical trials also impose an obstacle in the drug discovery pipeline. Artificial intelligence and machine learning technology play a crucial role in drug discovery and development. In other words, artificial neural networks and deep learning algorithms have modernized the area. Machine learning and deep learning algorithms have been implemented in several drug discovery processes such as peptide synthesis, structure-based virtual screening, ligand-based virtual screening, toxicity prediction, drug monitoring and release, pharmacophore modeling, quantitative structure–activity relationship, drug repositioning, polypharmacology, and physiochemical activity. Evidence from the past strengthens the implementation of artificial intelligence and deep learning in this field. Moreover, novel data mining, curation, and management techniques provided critical support to recently developed modeling algorithms. In summary, artificial intelligence and deep learning advancements provide an excellent opportunity for rational drug design and discovery process, which will eventually impact mankind. \\n', 'dataset': {'org-109.txt': 'Drug designing and development is an important area of research for pharmaceutical companies and chemical scientists. However, low efficacy, off-target delivery, time consumption, and high cost impose a hurdle and challenges that impact drug design and discovery. Further, complex and big data from genomics, proteomics, microarray data, and clinical trials also impose an obstacle in the drug discovery pipeline. Artificial intelligence and machine learning technology play a crucial role in drug discovery and development. In other words, artificial neural networks and deep learning algorithms have modernized the area. Machine learning and deep learning algorithms have been implemented in several drug discovery processes such as peptide synthesis, structure-based virtual screening, ligand-based virtual screening, toxicity prediction, drug monitoring and release, pharmacophore modeling, quantitative structure–activity relationship, drug repositioning, polypharmacology, and physiochemical activity. Evidence from the past strengthens the implementation of artificial intelligence and deep learning in this field. Moreover, novel data mining, curation, and management techniques provided critical support to recently developed modeling algorithms. In summary, artificial intelligence and deep learning advancements provide an excellent opportunity for rational drug design and discovery process, which will eventually impact mankind. '}}, 'FID-07.txt': {'suspicious': 'Human-AI interaction has become an important focus in developing technology that is more responsive and humane. In this context, the use of artificial empathy strategies is particularly interesting because of their potential to improve customer experiences affectively and socially. The aim of this research is to explore how artificial empathy strategies can optimize human-AI interactions and enhance affective and social customer experiences. The research approach is qualitative, involving a review of various studies and related literature. The data sources include journals, articles, and books relevant to the research topic. The research results indicate that implementing artificial empathy strategies in human-AI interactions has the potential to significantly improve the quality of interactions and customer experiences. Technologies such as natural language processing, emotion recognition, and sentiment analysis can enable AI to respond more precisely and sensitively to user needs and emotions.\\n', 'dataset': {'org-041.txt': 'Human-AI interaction has become an important focus in the development of more responsive and humane technology. In this context, the use of artificial empathy strategies is of particular interest due to its potential in improving customer experiences affectively and socially. This research aims to explore the optimization of human-AI interactions through the application of artificial empathy strategies in improving affective and social customer experiences. The research approach used is qualitative by reviewing various studies and related literature. The data sources used are journals, articles and books that are relevant to the research topic. From the research results, it was found that the implementation of artificial empathy strategies in human-AI interactions has great potential to improve the quality of interactions and customer experiences. The use of technologies such as natural language processing, emotion recognition, and sentiment analysis can enable AI to respond more precisely and sensitively to user needs and emotions.'}}, 'FID-10.txt': {'suspicious': 'The utilization of Artificial Intelligence (AI) technologies in education has surged, leading to a rise in published studies. Despite this, comprehensive large-scale reviews in this field are lacking. This study aims to bridge this gap by analyzing 4,519 publications from 2000 to 2019, using topic-based bibliometrics to identify trends and topics related to AI applications in education (AIEd). Results indicate a growing interest in using AI for educational purposes within the academic community. The primary research topics include intelligent tutoring systems for special education, natural language processing for language education, educational robots for AI education, educational data mining for performance prediction, discourse analysis in computer-supported collaborative learning, neural networks for teaching evaluation, affective computing for learner emotion detection, and recommender systems for personalized learning. The study also addresses the challenges and future directions of AIEd.\\n', 'dataset': {'org-007.txt': 'With the increasing use of Artificial Intelligence (AI) technologies in education, the number of published studies in the field has increased. However, no large-scale reviews have been conducted to comprehensively investigate the various aspects of this field. Based on 4,519 publications from 2000 to 2019, we attempt to fill this gap and identify trends and topics related to AI applications in education (AIEd) using topicbased bibliometrics. Results of the review reveal an increasing interest in using AI for educational purposes from the academic community. The main research topics include intelligent tutoring systems for special education; natural language processing for language education; educational robots for AI education; educational data mining for performance prediction; discourse analysis in computer-supported collaborative learning; neural networks for teaching evaluation; affective computing for learner emotion detection; and recommender systems for personalized learning. We also discuss the challenges and future directions of AIEd.\\n', 'org-002.txt': \"Artificial intelligence (AI) is developing and its application is spreading at an alarming rate, and AI has become part of our daily lives. As a matter of fact, AI has changed the way people learn. However, its adoption in the educational sector has been saddled with challenges and ethical issues. The purpose of this study is to analyze the opportunities, benefits, and challenges of AI in education. A review of available and relevant literature was done using the systematic review method to identify the current research focus and provide an in-depth understanding of AI technology in education for educators and future research directions. Findings showed that AI's adoption in education has advanced in the developed countries and most research became popular within the Industry 4.0 era. Other challenges, as well as recommendations, are discussed in the study.\\n\"}}, 'FID-08.txt': {'suspicious': \"\\ufeffThe main idea of this paper is the substantiation of the methodological approach to the assessment of personnel risks of enterprises based on the application of the fuzzy logic apparatus in order to identify the problems of personnel risk management and provide appropriate recommendations for their solution. The methodological basis of the study is the classic provisions and fundamental works of foreign and domestic scientists, statistical data, the results of our research into the problems of assessing personnel risks of enterprises. The methods of fuzzy set theory, comparative analysis, scientific abstraction, generalization of scientific experience of modern theoretical research, systemcomplex approach were used. The study proposed a methodological approach to assessing the level of personnel risks of an enterprise; numerical experiments were conducted on the basis of a group of construction equipment manufacturers. Analysis of the results of assessing the level of personnel risks of enterprises made it possible to identify the problems of managing personnel risks at enterprises Statement of a mathematical problem: the work considers hierarchical fuzzy data, namely: four groups of indicators for assessing the level of personnel risks (quantitative composition – F1, state of qualifications and intellectual potential – F2, staff turnover – F3, motivational system – F4), each of the indicators has a different number of fuzzy coefficients (there are twelve of them in the current work – vi , i=1÷12). Indicators are functions of fuzzy coefficients: F1 = r(v1, v2, v3); F2 = g(v4,v5, v6, v7); F3 = h(v8, v9, v10,); F4=q(v11, v12). As an output variable, there is a functional – an integrated indicator Int = f(F1, F2, F3, F4) of the personnel risk level, which, in turn, is also a fuzzy value. Here, the functions r, g, h, q, f are unknown functions of the given variables. We have expert evaluations of the change in all input data; as a rule, they vary within three terms: Low (I), Medium (G), High (E). Formalized information on each variable can be written as , then for a group of indicators we have: . Using a fuzzy system and performing calculations with its help requires the system to have the following structural elements: membership functions of input and output variables, a rule base, and an output mechanism. These structural elements are the components that will be built when designing a fuzzy system. The built mathematical model and the method of its formalization on the basis of FST make it possible to estimate the level of personnel risk at the enterprise, which enables further substantiation of a set of measures to increase the efficiency of its use. The constructed system of fuzzy logical inference can be considered intelligent as it uses elements of computational intelligence, in particular, the theory of fuzzy sets. The proposed methodological approach to assessing the level of personnel risks of enterprises based on the apparatus of fuzzy logic allows, in contrast to existing ones, to integrate the consideration of both qualitative and quantitative indicators when assessing the level of personnel risks and personnel movement indicators and to significantly increase the efficiency of decision-making under conditions of uncertainty and reduce costs in the event of adverse situations.This paper substantiates a methodological approach to assessing personnel risks in enterprises using fuzzy logic. The goal is to identify and provide recommendations for managing personnel risk problems. The study's foundation includes classic provisions of foreign and domestic scientists, statistical data, and the team's own research. Methods such as fuzzy set theory, comparative analysis, scientific abstraction, generalization of modern theoretical research, and a system-complex approach were employed. The study proposes a methodological approach to assessing personnel risk levels, exemplified by a group of construction equipment manufacturers. It considers hierarchical fuzzy data comprising four groups of indicators: quantitative composition (F1), state of qualifications and intellectual potential (F2), staff turnover (F3), and motivational system (F4). Each indicator has fuzzy coefficients (vi, i=1÷12), and the output variable is an integrated indicator Int, representing the personnel risk level. The paper introduces unknown functions r, g, h, q, f as functions of fuzzy coefficients, reflecting expert evaluations of input data changes (Low, Medium, High). The study utilizes a fuzzy system with structural elements including membership functions, a rule base, and an output mechanism. This system allows for estimating personnel risk levels and justifying measures to enhance efficiency. The fuzzy logical inference system is considered intelligent as it employs computational intelligence elements, particularly fuzzy set theory. This methodological approach integrates qualitative and quantitative indicators, enhancing decision-making efficiency under uncertainty and reducing costs during adverse situations, distinguishing it from existing methods.\\n\", 'dataset': {'org-079.txt': '\\ufeffThe main idea of this paper is the substantiation of the methodological approach to the assessment of personnel risks of enterprises based on the application of the fuzzy logic apparatus in order to identify the problems of personnel risk management and provide appropriate recommendations for their solution. The methodological basis of the study is the classic provisions and fundamental works of foreign and domestic scientists, statistical data, the results of our research into the problems of assessing personnel risks of enterprises. The methods of fuzzy set theory, comparative analysis, scientific abstraction, generalization of scientific experience of modern theoretical research, systemcomplex approach were used. The study proposed a methodological approach to assessing the level of personnel risks of an enterprise; numerical experiments were conducted on the basis of a group of construction equipment manufacturers. Analysis of the results of assessing the level of personnel risks of enterprises made it possible to identify the problems of managing personnel risks at enterprises Statement of a mathematical problem: the work considers hierarchical fuzzy data, namely: four groups of indicators for assessing the level of personnel risks (quantitative composition – F1, state of qualifications and intellectual potential – F2, staff turnover – F3, motivational system – F4), each of the indicators has a different number of fuzzy coefficients (there are twelve of them in the current work – vi , i=1÷12). Indicators are functions of fuzzy coefficients: F1 = r(v1, v2, v3); F2 = g(v4,v5, v6, v7); F3 = h(v8, v9, v10,); F4=q(v11, v12). As an output variable, there is a functional – an integrated indicator Int = f(F1, F2, F3, F4) of the personnel risk level, which, in turn, is also a fuzzy value. Here, the functions r, g, h, q, f are unknown functions of the given variables. We have expert evaluations of the change in all input data; as a rule, they vary within three terms: Low (I), Medium (G), High (E). Formalized information on each variable can be written as , then for a group of indicators we have: . Using a fuzzy system and performing calculations with its help requires the system to have the following structural elements: membership functions of input and output variables, a rule base, and an output mechanism. These structural elements are the components that will be built when designing a fuzzy system. The built mathematical model and the method of its formalization on the basis of FST make it possible to estimate the level of personnel risk at the enterprise, which enables further substantiation of a set of measures to increase the efficiency of its use. The constructed system of fuzzy logical inference can be considered intelligent as it uses elements of computational intelligence, in particular, the theory of fuzzy sets. The proposed methodological approach to assessing the level of personnel risks of enterprises based on the apparatus of fuzzy logic allows, in contrast to existing ones, to integrate the consideration of both qualitative and quantitative indicators when assessing the level of personnel risks and personnel movement indicators and to significantly increase the efficiency of decision-making under conditions of uncertainty and reduce costs in the event of adverse situations.'}}}\n"
          ]
        }
      ],
      "source": [
        "def compare_texts_with_dataset(suspicious_texts, dataset_texts, tokenizer, model, threshold=0.85):\n",
        "    '''\n",
        "    Esta función compara los textos sospechosos con los textos del dataset y devuelve\n",
        "    un diccionario con los textos sospechosos y los textos del dataset que superan un umbral de similitud.\n",
        "    Este diccionario en return es con el fin de poder servir como input para una función que genere un reporte del tipo de plagio encontrado.\n",
        "\n",
        "    Args:\n",
        "    suspicious_texts: list: lista de textos sospechosos\n",
        "    dataset_texts: list: lista de textos del dataset\n",
        "    tokenizer: tokenizer: tokenizador\n",
        "    model: model: modelo\n",
        "\n",
        "    Returns:\n",
        "    suspicious_and_dataset_files: dict: diccionario con los textos sospechosos y los textos del dataset que superan el umbral de similitud\n",
        "    '''\n",
        "\n",
        "    suspicious_and_dataset_files = {}\n",
        "\n",
        "    suspicious_embeddings = [get_mean_pooling_embeddings(text, tokenizer, model) for text in suspicious_texts]\n",
        "\n",
        "    dataset_embeddings = [get_mean_pooling_embeddings(text, tokenizer, model) for text in dataset_texts]\n",
        "\n",
        "    suspicious_embeddings = [emb.squeeze() for emb in suspicious_embeddings]\n",
        "    dataset_embeddings = [emb.squeeze() for emb in dataset_embeddings]\n",
        "\n",
        "    if suspicious_embeddings and dataset_embeddings:\n",
        "        suspicious_files = os.listdir(suspicious_folder)\n",
        "\n",
        "        for idx, suspicious_embedding in enumerate(suspicious_embeddings):\n",
        "            suspicious_filename = suspicious_files[idx]\n",
        "            suspicious_embedding = suspicious_embedding.reshape(1, -1)  # Convertir a matriz de una sola fila\n",
        "            similarities = cosine_similarity(suspicious_embedding, dataset_embeddings)\n",
        "\n",
        "            similarity_list = [(os.path.basename(dataset_file), similarity) for dataset_file, similarity in zip(os.listdir(dataset_folder), similarities.flatten())]\n",
        "            similarity_list.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Filtrar resultados por umbral\n",
        "            filtered_results = [(file, similarity) for file, similarity in similarity_list if similarity >= threshold]\n",
        "\n",
        "            if filtered_results:\n",
        "                suspicious_and_dataset_files[suspicious_filename] = {\n",
        "                    \"suspicious\": suspicious_texts[idx],\n",
        "                    \"dataset\": {file: dataset_texts[os.listdir(dataset_folder).index(file)] for file, similarity in filtered_results}\n",
        "                }\n",
        "\n",
        "    return suspicious_and_dataset_files\n",
        "\n",
        "result_dict = compare_texts_with_dataset(suspicious_texts, dataset_texts, tokenizer, model, threshold=0.85)\n",
        "\n",
        "print(result_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjcBLCXxh8sR"
      },
      "source": [
        "# Clasificación de texto con SpaCy\n",
        "\n",
        "En esta sección, se realiza la implementación de un clasificador de texto con la libreria SpaCy, la cual permite identificar similitud de texto tomando en cuenta: contexto, verbos, adjetivos y sustantivos.\n",
        "\n",
        "Esta herramienta la utilizamos para recibir un diccionario con los archivos que el modelo identificó como plagio, dividir el archivo sospechoso en distinas oraciones, al igual que los archivos con alta similitud de plagio. Posteriormente, cada oración sospechosa se compara con cada oración del org-000.txt. Identificando la oración con mayor similitud. Esta nos fue clave para poder identificar si fue plagio, parafraseado, cambio de voz o si no fue plagio. De la misma manera, se calucla un promedio con todas las oraciones para determinar qué tanta similitud hay en contexto, plabaras y oraciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDJMRCtsT_9w"
      },
      "source": [
        "Descargas e instalaciones de SpaCy y su modelo core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bi9B6XbVh6hJ",
        "outputId": "dc671848-fa3c-49c3-869d-cb7ab5ad8135"
      },
      "outputs": [],
      "source": [
        "#Instalación de spaCy\n",
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRS1-Zwkh_yE"
      },
      "outputs": [],
      "source": [
        "# descarga de un modelo pre-entrenado de spaCy para el inglés\n",
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrxxvqNSRz1c"
      },
      "outputs": [],
      "source": [
        "# carga del modelo en la varable nlp\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_lg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK5dG8R4T5YC"
      },
      "source": [
        "Procesamiento de textos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZxMlPEOiQiW"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    preprocesses_text es una función que recibe:\n",
        "        input: texto\n",
        "        output: texto procesado y tokenizado\n",
        "    Esta función usa el modelo nlp para tokenizar, asignar tags, eliminar\n",
        "    stops words y hacer lemmatizing.\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    processed_text = [token.lemma_ for token in doc if not token.is_stop]\n",
        "    return processed_text\n",
        "\n",
        "def calculate_similarity(text1, text2):\n",
        "    \"\"\"\n",
        "    calculate_similarity es una función que recibe:\n",
        "        input: texto sospechoso, texto posible mente plagiado\n",
        "        output: numero resultante del cálculo.\n",
        "    Esta función se encarga de comparar la similitud utilizando el modelo nlp\n",
        "    para comparar los textos recibidos y realizar un cálculo numerico.\n",
        "    \"\"\"\n",
        "    processed_text1 = preprocess_text(text1)\n",
        "    processed_text2 = preprocess_text(text2)\n",
        "\n",
        "    doc1 = nlp(\" \".join(processed_text1))\n",
        "    doc2 = nlp(\" \".join(processed_text2))\n",
        "\n",
        "    similarity_score = doc1.similarity(doc2)\n",
        "    return similarity_score\n",
        "\n",
        "def split_sentences(text):\n",
        "    \"\"\"\n",
        "    split_sentences es una función que recibe:\n",
        "        input: el texto de un abstract\n",
        "        output: oraciones separadas en una lista\n",
        "    Esta función recibe el abstract y lo divide usando spaCy de acuerdo a su\n",
        "    necesidad o enfoque.\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "    sentences = [sent.text for sent in doc.sents]\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VT-EE7r2UKJ8"
      },
      "source": [
        "Herramienta de calsificación de texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_ClBSUsiRZT"
      },
      "outputs": [],
      "source": [
        "def similarity_detector(result_dict, threshold_plagiarism, threshold_paraphrase, threshold_swap_phrases):\n",
        "    \"\"\"\n",
        "    similarity_detector es una función que recibe:\n",
        "        input: diccionario del modelo BERT, umbral para plagio, umbral para parafraseo y umbral para cambio de voz\n",
        "        output: numero resultante del cálculo.\n",
        "    Esta función recibe el diccionario del modelo BERT con las similitudes de plagio,\n",
        "    llama a las funciones split_sentences, calculate_similarity y preprocess_text,\n",
        "    los resultados los pasa por una comparación donde se realiza la clasificación de texto\n",
        "    para determinar qué tipo de similitud es.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    suspicious_and_dataset_files = result_dict\n",
        "\n",
        "    # Iterar sobre cada archivo sospechoso y sus respectivos archivos del dataset\n",
        "    for suspicious_file, data in suspicious_and_dataset_files.items():\n",
        "        suspicious_abstract = data[\"suspicious\"]\n",
        "        dataset_files = data[\"dataset\"]\n",
        "\n",
        "        result_for_suspicious_file = {}\n",
        "\n",
        "        # Iterar sobre cada archivo del dataset\n",
        "        for dataset_file, matched_abstract in dataset_files.items():\n",
        "            suspicious_sentences = split_sentences(suspicious_abstract)\n",
        "            matched_sentences = split_sentences(matched_abstract)\n",
        "\n",
        "            # Mapear las oraciones para conocer la ubicación de las oraciones con mayor similitud para ambos archivos\n",
        "            similarity_mapping = {}\n",
        "            for i, suspicious_sent in enumerate(suspicious_sentences):\n",
        "                similarity_mapping[i] = {}\n",
        "                for j, matched_sent in enumerate(matched_sentences):\n",
        "                    similarity_mapping[i][j] = calculate_similarity(suspicious_sent, matched_sent)\n",
        "\n",
        "            max_similarity_score = -1\n",
        "            max_similarity_indices = (-1, -1)\n",
        "            for i, scores in similarity_mapping.items():\n",
        "                for j, score in scores.items():\n",
        "                    if score > max_similarity_score:\n",
        "                        max_similarity_score = score\n",
        "                        max_similarity_indices = (i, j)\n",
        "                        break\n",
        "                else:\n",
        "                    continue\n",
        "                break\n",
        "\n",
        "            highest_similarity_sentence_suspicious = suspicious_sentences[max_similarity_indices[0]]\n",
        "            highest_similarity_sentence_matched = matched_sentences[max_similarity_indices[1]]\n",
        "\n",
        "            suspicious_average_similarity = sum(similarity_mapping[max_similarity_indices[0]].values()) / len(similarity_mapping[max_similarity_indices[0]])\n",
        "            matched_average_similarity = sum(similarity_mapping[max_similarity_indices[1]].values()) / len(similarity_mapping[max_similarity_indices[1]])\n",
        "\n",
        "            position_suspicious = max_similarity_indices[0]\n",
        "            position_matched = max_similarity_indices[1]\n",
        "\n",
        "            similarity_type = \"\"\n",
        "            if max_similarity_score >= threshold_plagiarism:\n",
        "                if position_suspicious != position_matched:\n",
        "                    similarity_type = \"Desorden de Frases\"\n",
        "                else:\n",
        "                    similarity_type = \"Plagio Completo\"\n",
        "            elif max_similarity_score >= threshold_paraphrase:\n",
        "                similarity_type = \"Parafraseo\"\n",
        "            elif max_similarity_score >= threshold_swap_phrases:\n",
        "                similarity_type = \"Cambio de voz\"\n",
        "            else:\n",
        "                similarity_type = \"No hay plagio\"\n",
        "\n",
        "            # Resultados de la herramienta\n",
        "            result_for_suspicious_file[dataset_file] = {\n",
        "                \"highest_similarity_score\": max_similarity_score,\n",
        "                \"similarity_type\": similarity_type,\n",
        "                \"suspicious_average_similarity\": suspicious_average_similarity,\n",
        "                \"matched_average_similarity\": matched_average_similarity,\n",
        "                \"position_suspicious\": position_suspicious,\n",
        "                \"position_matched\": position_matched,\n",
        "                \"matched_original_text\": highest_similarity_sentence_matched,\n",
        "                \"matched_suspicious_text\": highest_similarity_sentence_suspicious\n",
        "            }\n",
        "\n",
        "            if similarity_type == \"Plagio Completo\":\n",
        "                break\n",
        "\n",
        "        results[suspicious_file] = result_for_suspicious_file\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXOEWqhJZ23m"
      },
      "source": [
        "Impresión de los resultados, validación de los umbrales para la clasificación y llamada a la función similarity_detector.\n",
        "\n",
        "La lógica detrás de los umbrales fue la siguiente:\n",
        "* Para el umbral de plagio completo, se fijó en 0.99. Esto se debe a que, en casos de plagio completo, las oraciones comparadas pueden presentar similitud del 100%. Sin embargo, en algunas ocasiones, debido a la división de oraciones, podría resultar en un 99%. Por lo tanto, se estableció este umbral con un ligero margen de seguridad.\n",
        "\n",
        "* El umbral para parafraseo se estableció en 0.87. Esto se debió a que, si bien el parafraseo puede reducir la similitud, la captura del contexto puede generar similitudes altas. Por lo tanto, se asignó un margen de 0.87 a 0.98 para abarcar diversas variaciones de parafraseo.\n",
        "* Se fijó el umbral para cambio de voz en 0.77. Este umbral se determinó considerando que el cambio de voz disminuía la similitud de manera significativa. Sin embargo, el contexto puro aún elevaba el promedio a alrededor de 77-79.\n",
        "\n",
        "Se consideró que cualquier promedio menor a 0.77 no constituía plagio. Esto se basó en la observación de que, en archivos que trataban temas de tecnología, medicina, IA y Machine Learning, el promedio de similitud era consistentemente superior a 0.70. Sin embargo, en casos de verdadera similitud, los porcentajes de similitud eran mucho más altos y notorios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPsV7iGci2Wo"
      },
      "outputs": [],
      "source": [
        "threshold_plagiarism = 0.99\n",
        "threshold_paraphrase = 0.87\n",
        "threshold_swap_phrases = 0.77\n",
        "\n",
        "results = similarity_detector(result_dict, threshold_plagiarism, threshold_paraphrase, threshold_swap_phrases)\n",
        "\n",
        "print(results)\n",
        "\n",
        "for suspicious_file, dataset_results in results.items():\n",
        "    print(f\"\\nResults for {suspicious_file}:\")\n",
        "    for dataset_file, result in dataset_results.items():\n",
        "        print(f\"\\n\\tDataset file: {dataset_file}\")\n",
        "        print(f\"\\tHighest similarity score: {result['highest_similarity_score']}\")\n",
        "        print(f\"\\tSimilarity type: {result['similarity_type']}\")\n",
        "        print(f\"\\tPosition suspicious: {result['position_suspicious']}\")\n",
        "        print(f\"\\tPosition matched: {result['position_matched']}\")\n",
        "        print(f\"\\tMatched Sentence from original text: {result['matched_original_text']}\")\n",
        "        print(f\"\\tMatched Sentence from suspicious text: {result['matched_suspicious_text']}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
